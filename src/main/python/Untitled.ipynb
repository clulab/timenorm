{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ffac397e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/bulut/miniforge3/envs/timenorm/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Reusing dataset conll2003 (/Users/bulut/.cache/huggingface/datasets/conll2003/conll2003/1.0.0/63f4ebd1bcb7148b1644497336fd74643d4ce70123334431a3c053b7ee4e96ee)\n",
      "100%|████████████████████████████████████████████████████████████████████| 3/3 [00:00<00:00, 313.87it/s]\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset, Dataset, DatasetDict\n",
    "from transformers import AutoTokenizer\n",
    "from timenorm_data_provider import TimeDataProvider\n",
    "from transformers import RobertaForTokenClassification\n",
    "from utils_ import *\n",
    "import datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "aff91247",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1236 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./example-data/Train/ID032_clinic_094/ID032_clinic_094\n",
      "./example-data/Train/ID027_path_080/ID027_path_080\n",
      "./example-data/Train/ID025_clinic_073/ID025_clinic_073\n",
      "the entity that is linked does not exist in the document\n",
      "0/116 is done\n",
      "Skipping token types: 'ment [today] with' {'Calendar-Interval', 'This'}\n",
      "Skipping token types: 's of [Friday], Sep' {'Intersection', 'Day-Of-Week'}\n",
      "Skipping token types: 's of [Friday], Sep' {'Intersection', 'Day-Of-Week'}\n",
      "Skipping token types: 'Rate=[20] /min' {'Frequency', 'Number'}\n",
      "Skipping token types: 'Rate=[78] /min' {'Frequency', 'Number'}\n",
      "Skipping token types: 'r to [today]).  A' {'Calendar-Interval', 'This'}\n",
      "Skipping token types: 'that [today] or f' {'Calendar-Interval', 'This'}\n",
      "Skipping token types: 'hing [tomorrow] morn' {'Calendar-Interval', 'Next', 'Intersection'}\n",
      "Skipping token types: 'gery [tomorrow].  Sh' {'Calendar-Interval', 'Next'}\n",
      "100/116 is done\n",
      "Skipping token types: 's of [Wednesday], Apr' {'Intersection', 'Day-Of-Week'}\n",
      "Skipping token types: 's of [Wednesday], Apr' {'Intersection', 'Day-Of-Week'}\n",
      "Skipping token types: 'Rate=[16] /min' {'Frequency', 'Number'}\n",
      "Skipping token types: 'Rate=[66] /min' {'Frequency', 'Number'}\n",
      "./example-data/Dev/ID012_clinic_036/ID012_clinic_036\n",
      "./example-data/Dev/ID004_clinic_012/ID004_clinic_012\n",
      "./example-data/Dev/ID004_path_011/ID004_path_011\n",
      "./example-data/Dev/ID004_clinic_010/ID004_clinic_010\n",
      "./example-data/Dev/ID012_clinic_034/ID012_clinic_034\n",
      "0/385 is done\n",
      "Skipping token labels: 'I on [November] 17 d' {'-1', 'None'}\n",
      "Skipping token types: 'I on [November] 17 d' {'Month-Of-Year', 'Last'}\n",
      "Skipping token types: 'here [today] afte' {'Calendar-Interval', 'This'}\n",
      "Skipping token types: 's of [Thursday], Oct' {'Intersection', 'Day-Of-Week'}\n",
      "Skipping token types: ' age [55].\\n\\n[e' {'Period', 'Number'}\n",
      "Skipping token types: 'Rate=[93] /min' {'Frequency', 'Number'}\n",
      "Skipping token types: 'gist [today] woul' {'Calendar-Interval', 'This'}\n",
      "100/385 is done\n",
      "Skipping token labels: '.\\n3. [June] 16 t' {'-1', 'None'}\n",
      "Skipping token types: '.\\n3. [June] 16 t' {'Month-Of-Year', 'Last'}\n",
      "Skipping token types: 'ound [twice]-a-da' {'Frequency', 'Number'}\n",
      "Skipping token types: 'nine [today] of 2' {'Calendar-Interval', 'This'}\n",
      "Skipping token types: 'logy [today] who ' {'Calendar-Interval', 'This'}\n",
      "Skipping token types: 'cked [twice]-a-da' {'Frequency', 'Number'}\n",
      "Skipping token types: ' age [15]. RUE' {'Period', 'Number'}\n",
      "Skipping token types: 's of [Tuesday], Jul' {'Intersection', 'Day-Of-Week'}\n",
      "Skipping token types: 'zole [cream] topi' {'Frequency', 'Number'}\n",
      "Skipping token types: 's of [Tuesday], Jul' {'Intersection', 'Day-Of-Week'}\n",
      "Skipping token types: 'ined [today] incl' {'Calendar-Interval', 'This'}\n",
      "Skipping token types: \" for [today]'s co\" {'Calendar-Interval', 'This'}\n",
      "Skipping token types: \" for [today]'s co\" {'Calendar-Interval', 'This'}\n",
      "Skipping token types: \" for [today]'s co\" {'Calendar-Interval', 'This'}\n",
      "200/385 is done\n",
      "Skipping token types: ' age [15]. RUE' {'Period', 'Number'}\n",
      "Skipping token types: 's of [Monday], Jun' {'Intersection', 'Day-Of-Week'}\n",
      "Skipping token types: 'zole [cream] topi' {'Frequency', 'Number'}\n",
      "Skipping token types: 'ater [today].  We' {'Calendar-Interval', 'This'}\n",
      "300/385 is done\n",
      "Skipping token types: '.\\n2. [August] thro' {'Month-Of-Year', 'Last'}\n",
      "Skipping token types: 'n of [July] whic' {'Month-Of-Year', 'Last'}\n",
      "Skipping token types: 's of [Thursday], Jun' {'Intersection', 'Day-Of-Week'}\n",
      "Skipping token types: ' age [55].\\n\\n[e' {'Period', 'Number'}\n",
      "Skipping token types: 'Rate=[76] /min' {'Frequency', 'Number'}\n",
      "./example-data/Test/ID007_clinic_021/ID007_clinic_021\n",
      "./example-data/Test/ID007_clinic_019/ID007_clinic_019\n",
      "./example-data/Test/ID006_path_017a/ID006_path_017a\n",
      "./example-data/Test/ID014_clinic_040/ID014_clinic_040\n",
      "./example-data/Test/ID006_clinic_018/ID006_clinic_018\n",
      "./example-data/Test/ID007_path_020/ID007_path_020\n",
      "./example-data/Test/ID006_clinic_016/ID006_clinic_016\n",
      "0/512 is done\n",
      "Skipping token types: 'NX).\\n[Today] Mrs.' {'Calendar-Interval', 'This'}\n",
      "Skipping token types: 's of [Monday], Nov' {'Intersection', 'Day-Of-Week'}\n",
      "Skipping token types: 's of [Monday], Nov' {'Intersection', 'Day-Of-Week'}\n",
      "100/512 is done\n",
      "Skipping token types: 'Rate=[95] /min' {'Frequency', 'Number'}\n",
      "Skipping token types: 'ined [today].\\nDis' {'Calendar-Interval', 'This'}\n",
      "Skipping token types: 'ince [April] this' {'Month-Of-Year', 'Intersection'}\n",
      "Skipping token types: '5 in [September].  Sh' {'Month-Of-Year', 'Last'}\n",
      "Skipping token types: ' the [day] on O' {'Calendar-Interval', 'Intersection'}\n",
      "Skipping token labels: 'tion [October] 13. ' {'-1', 'None'}\n",
      "Skipping token types: 'tion [October] 13. ' {'Month-Of-Year', 'Last'}\n",
      "Skipping token labels: 'ntil [October] 16. ' {'-1', 'None'}\n",
      "Skipping token types: 'ntil [October] 16. ' {'Month-Of-Year', 'Last'}\n",
      "Skipping token labels: ' the [September] 22 e' {'-1', 'None'}\n",
      "Skipping token types: ' the [September] 22 e' {'Month-Of-Year', 'Last'}\n",
      "Skipping token types: 's of [Wednesday], Oct' {'Intersection', 'Day-Of-Week'}\n",
      "Skipping token types: 'late [40]s, ea' {'Period', 'Number'}\n",
      "Skipping token types: 'te 40[s], ear' {'Period', 'Number'}\n",
      "Skipping token types: 'arly [50]s.  A' {'Period', 'Number'}\n",
      "Skipping token types: 'ly 50[s].  A ' {'Period', 'Number'}\n",
      "Skipping token types: '2-mg [tablet] by m' {'Frequency', 'Number'}\n",
      "Skipping token types: '6 PM [the] even' {'Intersection', 'This'}\n",
      "Skipping token types: 'ning [before] your' {'Before', 'Intersection'}\n",
      "200/512 is done\n",
      "Skipping token types: 's of [Wednesday], Oct' {'Intersection', 'Day-Of-Week'}\n",
      "Skipping token types: 'ital [today].  I ' {'Calendar-Interval', 'This'}\n",
      "Skipping token labels: 'e on [October] 22 w' {'-1', 'None'}\n",
      "Skipping token types: 'e on [October] 22 w' {'Month-Of-Year', 'Last'}\n",
      "Skipping token labels: 'd on [October] 23 w' {'-1', 'None'}\n",
      "Skipping token types: 'd on [October] 23 w' {'Month-Of-Year', 'Last'}\n",
      "Skipping token labels: 'o on [October] 21. ' {'-1', 'None'}\n",
      "Skipping token types: 'o on [October] 21. ' {'Month-Of-Year', 'Last'}\n",
      "Skipping token types: ' 1.3 [today].  Pa' {'Calendar-Interval', 'This'}\n",
      "Skipping token types: 'Rate=[105] /min' {'Frequency', 'Number'}\n",
      "300/512 is done\n",
      "Skipping token types: 'gist [today] woul' {'Calendar-Interval', 'This'}\n",
      "Skipping token types: ' him [today] to d' {'Calendar-Interval', 'This'}\n",
      "Skipping token types: 'ics.\\n[Today] Mr. ' {'Calendar-Interval', 'This'}\n",
      "Skipping token types: 's of [Tuesday], Aug' {'Intersection', 'Day-Of-Week'}\n",
      "Skipping token types: ' her [60]s.\\nMa' {'Period', 'Number'}\n",
      "Skipping token types: 'er 60[s].\\nMat' {'Period', 'Number'}\n",
      "Skipping token types: ' mid-[50]s.\\nSi' {'Period', 'Number'}\n",
      "Skipping token types: 'id-50[s].\\nSis' {'Period', 'Number'}\n",
      "Skipping token types: ' age [58].\\nPat' {'Period', 'Number'}\n",
      "Skipping token types: 'heir [50]s.\\n\\n[' {'Period', 'Number'}\n",
      "Skipping token types: 'ir 50[s].\\n\\n[e' {'Period', 'Number'}\n",
      "Skipping token types: 's of [Tuesday], Aug' {'Intersection', 'Day-Of-Week'}\n",
      "Skipping token types: 'Rate=[93] /min' {'Frequency', 'Number'}\n",
      "Skipping token types: 'ined [today] incl' {'Calendar-Interval', 'This'}\n",
      "Skipping token types: \" for [today]'s co\" {'Calendar-Interval', 'This'}\n",
      "Skipping token types: \" for [today]'s co\" {'Calendar-Interval', 'This'}\n",
      "400/512 is done\n",
      "Skipping token labels: 'l of [2010], had' {'4', '2'}\n",
      "Skipping token types: 'e in [March] and ' {'Month-Of-Year', 'Last'}\n",
      "Skipping token types: ' and [April] were' {'Month-Of-Year', 'Last'}\n",
      "Skipping token types: 'e of [55] whic' {'Period', 'Number'}\n",
      "Skipping token types: 's of [Tuesday], Jun' {'Intersection', 'Day-Of-Week'}\n",
      "Skipping token types: ' her [50]s.  B' {'Period', 'Number'}\n",
      "Skipping token types: 'er 50[s].  Br' {'Period', 'Number'}\n",
      "Skipping token types: 's of [Tuesday], Jun' {'Intersection', 'Day-Of-Week'}\n",
      "Skipping token types: 'Rate=[63] /min' {'Frequency', 'Number'}\n",
      "500/512 is done\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"roberta-base\", use_fast=True)\n",
    "data_provider = TimeDataProvider(corpus_dir=\"./example-data\")\n",
    "relation_to_extract = 'Sub-Interval'\n",
    "distances = [\n",
    "    '-6',\n",
    "    '-5',\n",
    "    '-4',\n",
    "    '-3',\n",
    "    '-2',\n",
    "    '-1',\n",
    "    'None',\n",
    "    '1',\n",
    "    '2',\n",
    "    '3',\n",
    "    '4',\n",
    "]\n",
    "f = open('./example-data/types.txt')\n",
    "lines = f.readlines()\n",
    "types = []\n",
    "for line in lines:\n",
    "    types.append(line.replace(\"\\n\", \"\"))\n",
    "tokenized_dataset = data_provider.read_data_to_distance_format(tokenizer, relation_to_extract, distances, types)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "27b75248",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['input_ids', 'labels_type', 'labels_distance'],\n",
       "        num_rows: 116\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['input_ids', 'labels_type', 'labels_distance'],\n",
       "        num_rows: 385\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['input_ids', 'labels_type', 'labels_distance'],\n",
       "        num_rows: 512\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c87abd1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dict = {\"input_ids\": input_ids, \"labels_type\": labels_type}\n",
    "train_dataset = Dataset.from_dict(train_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "9e014949",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['input_ids', 'labels_type'],\n",
       "    num_rows: 116\n",
       "})"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b182a6b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([116, 118])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.tensor(train_dataset['input_ids']).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41844092",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_dataset = DatasetDict(dataset_dict)\n",
    "temp_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fbb97a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_dataset['train']['a']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec5dc9b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "task_name = 'pos'\n",
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"conll2003\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43c4f387",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_dataset = prepare_dataset(dataset, task_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80461b58",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_dataset['train']['input_ids']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90447ca9",
   "metadata": {},
   "outputs": [],
   "source": [
    "shuffled_train_dataset = tokenized_dataset[\"train\"].shuffle(seed=42).select(range(len(tokenized_dataset[\"train\"])))\n",
    "train_dataloader = DataLoader(shuffled_train_dataset, batch_size=train_batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9d9997bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(tokenized_dataset[\"train\"], batch_size=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27b25ff2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"conll2003\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e661ca2",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_dataset['train']['labels'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5da8c9ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_dataset['train']['input_ids'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b4d6bb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_list = dataset[\"train\"].features[f\"{task_name}_tags\"].feature.names\n",
    "num_labels = len(label_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "751c5bf3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForTokenClassification: ['lm_head.layer_norm.weight', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.bias', 'lm_head.decoder.weight']\n",
      "- This IS expected if you are initializing RobertaForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = RobertaForTokenClassification.from_pretrained(pretrained_model_name_or_path='roberta-base', \n",
    "                                                                num_labels=11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ca02fd5b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RobertaForTokenClassification(\n",
       "  (roberta): RobertaModel(\n",
       "    (embeddings): RobertaEmbeddings(\n",
       "      (word_embeddings): Embedding(50265, 768, padding_idx=1)\n",
       "      (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
       "      (token_type_embeddings): Embedding(1, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): RobertaEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (6): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (7): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (8): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (9): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (10): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (11): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=11, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a6880af",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "01afb62b",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "expected scalar type Long but found Double",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Input \u001b[0;32mIn [29]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m model\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[1;32m      3\u001b[0m batch \u001b[38;5;241m=\u001b[39m {k: torch\u001b[38;5;241m.\u001b[39mstack(v)\u001b[38;5;241m.\u001b[39mto(device) \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m batch\u001b[38;5;241m.\u001b[39mitems()}\n\u001b[0;32m----> 5\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43minput_ids\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mlabels_distance\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28mprint\u001b[39m(outputs)\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[0;32m~/miniforge3/envs/timenorm/lib/python3.9/site-packages/transformers/models/roberta/modeling_roberta.py:1426\u001b[0m, in \u001b[0;36mRobertaForTokenClassification.forward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1424\u001b[0m         loss \u001b[38;5;241m=\u001b[39m loss_fct(active_logits, active_labels)\n\u001b[1;32m   1425\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1426\u001b[0m         loss \u001b[38;5;241m=\u001b[39m \u001b[43mloss_fct\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlogits\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mview\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnum_labels\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mview\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1428\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m return_dict:\n\u001b[1;32m   1429\u001b[0m     output \u001b[38;5;241m=\u001b[39m (logits,) \u001b[38;5;241m+\u001b[39m outputs[\u001b[38;5;241m2\u001b[39m:]\n",
      "File \u001b[0;32m~/miniforge3/envs/timenorm/lib/python3.9/site-packages/torch/nn/modules/module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1106\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1107\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1108\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1111\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1112\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniforge3/envs/timenorm/lib/python3.9/site-packages/torch/nn/modules/loss.py:1163\u001b[0m, in \u001b[0;36mCrossEntropyLoss.forward\u001b[0;34m(self, input, target)\u001b[0m\n\u001b[1;32m   1162\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor, target: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m-> 1163\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcross_entropy\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1164\u001b[0m \u001b[43m                           \u001b[49m\u001b[43mignore_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mignore_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreduction\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreduction\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1165\u001b[0m \u001b[43m                           \u001b[49m\u001b[43mlabel_smoothing\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlabel_smoothing\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniforge3/envs/timenorm/lib/python3.9/site-packages/torch/nn/functional.py:2996\u001b[0m, in \u001b[0;36mcross_entropy\u001b[0;34m(input, target, weight, size_average, ignore_index, reduce, reduction, label_smoothing)\u001b[0m\n\u001b[1;32m   2994\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m size_average \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m reduce \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   2995\u001b[0m     reduction \u001b[38;5;241m=\u001b[39m _Reduction\u001b[38;5;241m.\u001b[39mlegacy_get_string(size_average, reduce)\n\u001b[0;32m-> 2996\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_C\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_nn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcross_entropy_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_Reduction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_enum\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreduction\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mignore_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabel_smoothing\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: expected scalar type Long but found Double"
     ]
    }
   ],
   "source": [
    "for batch in train_dataloader:\n",
    "    model.train()\n",
    "    batch = {k: torch.stack(v).to(device) for k, v in batch.items()}\n",
    "    \n",
    "    outputs = model.forward(input_ids=batch['input_ids'], labels=batch['labels_distance'])\n",
    "    print(outputs)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f1d224a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0., 0., 0., 0.], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "for batch in train_dataloader:\n",
    "    print(batch['labels_type'][0])\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a953b6c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([0., 0., 0., 0.], dtype=torch.float64),\n",
       " tensor([0., 0., 0., 0.], dtype=torch.float64),\n",
       " tensor([0., 0., 0., 0.], dtype=torch.float64),\n",
       " tensor([0., 0., 0., 0.], dtype=torch.float64),\n",
       " tensor([0., 0., 0., 0.], dtype=torch.float64),\n",
       " tensor([0., 0., 0., 0.], dtype=torch.float64),\n",
       " tensor([0., 0., 0., 0.], dtype=torch.float64),\n",
       " tensor([4., 0., 0., 0.], dtype=torch.float64),\n",
       " tensor([0., 0., 0., 0.], dtype=torch.float64),\n",
       " tensor([5., 0., 0., 0.], dtype=torch.float64),\n",
       " tensor([0., 0., 0., 0.], dtype=torch.float64),\n",
       " tensor([2., 0., 0., 0.], dtype=torch.float64),\n",
       " tensor([0., 0., 0., 0.], dtype=torch.float64),\n",
       " tensor([0., 0., 0., 0.], dtype=torch.float64),\n",
       " tensor([0., 0., 0., 0.], dtype=torch.float64),\n",
       " tensor([0., 0., 0., 0.], dtype=torch.float64),\n",
       " tensor([0., 0., 0., 0.], dtype=torch.float64),\n",
       " tensor([4., 0., 0., 0.], dtype=torch.float64),\n",
       " tensor([0., 0., 0., 0.], dtype=torch.float64),\n",
       " tensor([5., 0., 0., 0.], dtype=torch.float64),\n",
       " tensor([0., 0., 0., 0.], dtype=torch.float64),\n",
       " tensor([2., 0., 0., 0.], dtype=torch.float64),\n",
       " tensor([0., 0., 0., 0.], dtype=torch.float64),\n",
       " tensor([0., 0., 0., 0.], dtype=torch.float64),\n",
       " tensor([0., 0., 0., 0.], dtype=torch.float64),\n",
       " tensor([0., 0., 0., 0.], dtype=torch.float64),\n",
       " tensor([0., 0., 0., 0.], dtype=torch.float64),\n",
       " tensor([0., 0., 0., 0.], dtype=torch.float64),\n",
       " tensor([0., 0., 0., 0.], dtype=torch.float64),\n",
       " tensor([0., 0., 0., 0.], dtype=torch.float64),\n",
       " tensor([0., 0., 0., 0.], dtype=torch.float64),\n",
       " tensor([0., 0., 0., 0.], dtype=torch.float64),\n",
       " tensor([0., 0., 0., 0.], dtype=torch.float64),\n",
       " tensor([0., 0., 0., 0.], dtype=torch.float64),\n",
       " tensor([0., 0., 0., 0.], dtype=torch.float64),\n",
       " tensor([0., 0., 0., 0.], dtype=torch.float64),\n",
       " tensor([0., 0., 0., 0.], dtype=torch.float64),\n",
       " tensor([0., 0., 0., 0.], dtype=torch.float64),\n",
       " tensor([0., 0., 0., 0.], dtype=torch.float64),\n",
       " tensor([0., 0., 0., 0.], dtype=torch.float64),\n",
       " tensor([0., 0., 0., 0.], dtype=torch.float64),\n",
       " tensor([0., 0., 0., 0.], dtype=torch.float64),\n",
       " tensor([0., 0., 0., 0.], dtype=torch.float64),\n",
       " tensor([0., 0., 0., 0.], dtype=torch.float64),\n",
       " tensor([0., 0., 0., 0.], dtype=torch.float64),\n",
       " tensor([0., 0., 0., 0.], dtype=torch.float64),\n",
       " tensor([0., 0., 0., 0.], dtype=torch.float64),\n",
       " tensor([0., 0., 0., 0.], dtype=torch.float64),\n",
       " tensor([0., 0., 0., 0.], dtype=torch.float64),\n",
       " tensor([0., 0., 0., 0.], dtype=torch.float64),\n",
       " tensor([0., 0., 0., 0.], dtype=torch.float64),\n",
       " tensor([0., 0., 0., 0.], dtype=torch.float64),\n",
       " tensor([0., 0., 0., 0.], dtype=torch.float64),\n",
       " tensor([0., 0., 0., 0.], dtype=torch.float64),\n",
       " tensor([0., 0., 0., 0.], dtype=torch.float64),\n",
       " tensor([0., 0., 0., 0.], dtype=torch.float64),\n",
       " tensor([0., 0., 0., 0.], dtype=torch.float64),\n",
       " tensor([0., 0., 0., 0.], dtype=torch.float64),\n",
       " tensor([0., 0., 0., 0.], dtype=torch.float64),\n",
       " tensor([0., 0., 0., 0.], dtype=torch.float64),\n",
       " tensor([0., 0., 0., 0.], dtype=torch.float64),\n",
       " tensor([0., 0., 0., 0.], dtype=torch.float64),\n",
       " tensor([0., 0., 0., 0.], dtype=torch.float64),\n",
       " tensor([0., 0., 0., 0.], dtype=torch.float64),\n",
       " tensor([0., 0., 0., 0.], dtype=torch.float64),\n",
       " tensor([0., 0., 0., 0.], dtype=torch.float64),\n",
       " tensor([0., 0., 0., 0.], dtype=torch.float64),\n",
       " tensor([0., 0., 0., 0.], dtype=torch.float64),\n",
       " tensor([0., 0., 0., 0.], dtype=torch.float64),\n",
       " tensor([0., 0., 0., 0.], dtype=torch.float64),\n",
       " tensor([0., 0., 0., 0.], dtype=torch.float64),\n",
       " tensor([0., 0., 0., 0.], dtype=torch.float64),\n",
       " tensor([0., 0., 0., 0.], dtype=torch.float64),\n",
       " tensor([0., 0., 0., 0.], dtype=torch.float64),\n",
       " tensor([0., 0., 0., 0.], dtype=torch.float64),\n",
       " tensor([0., 0., 0., 0.], dtype=torch.float64),\n",
       " tensor([0., 0., 0., 0.], dtype=torch.float64),\n",
       " tensor([0., 0., 0., 0.], dtype=torch.float64),\n",
       " tensor([0., 0., 0., 0.], dtype=torch.float64),\n",
       " tensor([0., 0., 0., 0.], dtype=torch.float64),\n",
       " tensor([0., 0., 0., 0.], dtype=torch.float64),\n",
       " tensor([0., 0., 0., 0.], dtype=torch.float64),\n",
       " tensor([0., 0., 0., 0.], dtype=torch.float64),\n",
       " tensor([0., 0., 0., 0.], dtype=torch.float64),\n",
       " tensor([0., 0., 0., 0.], dtype=torch.float64),\n",
       " tensor([0., 0., 0., 0.], dtype=torch.float64),\n",
       " tensor([0., 0., 0., 0.], dtype=torch.float64),\n",
       " tensor([0., 0., 0., 0.], dtype=torch.float64),\n",
       " tensor([0., 0., 0., 0.], dtype=torch.float64),\n",
       " tensor([0., 0., 0., 0.], dtype=torch.float64),\n",
       " tensor([0., 0., 0., 0.], dtype=torch.float64),\n",
       " tensor([0., 0., 0., 0.], dtype=torch.float64),\n",
       " tensor([0., 0., 0., 0.], dtype=torch.float64),\n",
       " tensor([0., 0., 0., 0.], dtype=torch.float64),\n",
       " tensor([0., 0., 0., 0.], dtype=torch.float64),\n",
       " tensor([0., 0., 0., 0.], dtype=torch.float64),\n",
       " tensor([0., 0., 0., 0.], dtype=torch.float64),\n",
       " tensor([0., 0., 0., 0.], dtype=torch.float64),\n",
       " tensor([0., 0., 0., 0.], dtype=torch.float64),\n",
       " tensor([0., 0., 0., 0.], dtype=torch.float64),\n",
       " tensor([0., 0., 0., 0.], dtype=torch.float64),\n",
       " tensor([0., 0., 0., 0.], dtype=torch.float64),\n",
       " tensor([0., 0., 0., 0.], dtype=torch.float64),\n",
       " tensor([0., 0., 0., 0.], dtype=torch.float64),\n",
       " tensor([0., 0., 0., 0.], dtype=torch.float64),\n",
       " tensor([0., 0., 0., 0.], dtype=torch.float64),\n",
       " tensor([0., 0., 0., 0.], dtype=torch.float64),\n",
       " tensor([0., 0., 0., 0.], dtype=torch.float64),\n",
       " tensor([0., 0., 0., 0.], dtype=torch.float64),\n",
       " tensor([0., 0., 0., 0.], dtype=torch.float64),\n",
       " tensor([0., 0., 0., 0.], dtype=torch.float64),\n",
       " tensor([0., 0., 0., 0.], dtype=torch.float64),\n",
       " tensor([0., 0., 0., 0.], dtype=torch.float64),\n",
       " tensor([0., 0., 0., 0.], dtype=torch.float64),\n",
       " tensor([0., 0., 0., 0.], dtype=torch.float64),\n",
       " tensor([0., 0., 0., 0.], dtype=torch.float64),\n",
       " tensor([0., 0., 0., 0.], dtype=torch.float64),\n",
       " tensor([0., 0., 0., 0.], dtype=torch.float64)]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch['labels_type']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ce861609",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0.],\n",
       "        [4., 0., 0., 0.],\n",
       "        [0., 0., 0., 0.],\n",
       "        [5., 0., 0., 0.],\n",
       "        [0., 0., 0., 0.],\n",
       "        [2., 0., 0., 0.],\n",
       "        [0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0.],\n",
       "        [4., 0., 0., 0.],\n",
       "        [0., 0., 0., 0.],\n",
       "        [5., 0., 0., 0.],\n",
       "        [0., 0., 0., 0.],\n",
       "        [2., 0., 0., 0.],\n",
       "        [0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0.]], dtype=torch.float64)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.stack(batch['labels_type'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8707fca",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn import CrossEntropyLoss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d7fc979",
   "metadata": {},
   "outputs": [],
   "source": [
    "b_logits = outputs.logits\n",
    "a_labels = batch['labels']\n",
    "c_weights = torch.rand(a_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8812ea6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "a_labels = torch.reshape(a_labels, (-1,))\n",
    "b_logits = torch.reshape(b_logits, (-1,b_logits.shape[-1]))\n",
    "c_weights = c_weights.to(b_logits.dtype)\n",
    "c_weights = torch.reshape(c_weights, (-1,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f0a51de",
   "metadata": {},
   "outputs": [],
   "source": [
    "c_weights.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0adc3dd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "b_logits.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc9fcbbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fct = CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04cb98ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fct(b_logits, a_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d5c1b66",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fct(b_logits, a_labels)*c_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4aba054",
   "metadata": {},
   "outputs": [],
   "source": [
    "distances = [\n",
    "    '-6',\n",
    "    '-5',\n",
    "    '-4',\n",
    "    '-3',\n",
    "    '-2',\n",
    "    '-1',\n",
    "    'None',\n",
    "    '1',\n",
    "    '2',\n",
    "    '3',\n",
    "    '4',\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a87008d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = [0 if d=='None' else 500 for d in distances]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5279e6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91a6a0e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 3\n",
    "learning_rate = 2e-5\n",
    "batch_size = 32\n",
    "model_name = \"roberta-base\"\n",
    "output_path = f\"/xdisk/bethard/kbozler/timenorm/relation-extraction/method2/{model_name}/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c1b0f48",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af161b65",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.path.join(output_path, f\"{num_epochs}epochs\", f\"{learning_rate}learning_rate\", f\"{batch_size}batch_size\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b41a8864",
   "metadata": {},
   "outputs": [],
   "source": [
    "features = datasets.Features({'a':list})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7f8ee3b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
